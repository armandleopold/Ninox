From 15ad820078533690cb202cfa1418c6d874c65c15 Mon Sep 17 00:00:00 2001
From: Armand Leopold <armand.leopold@hotmail.fr>
Date: Mon, 30 Jan 2017 16:52:16 +0100
Subject: [PATCH] create link between kafka and spark stream

---
 Dockerfile/ninox.sh                      |   5 ++---
 Spark_Stream/kafka_spark_mongodb.py      |  11 +++++++----
 Spark_Stream/run.sh                      |  16 +++++++++++-----
 kafka_custom_producer/DataProducer.class | Bin 2213 -> 2484 bytes
 kafka_custom_producer/DataProducer.java  |   9 ++++++++-
 5 files changed, 28 insertions(+), 13 deletions(-)

diff --git a/Dockerfile/ninox.sh b/Dockerfile/ninox.sh
index 6811ee1..4d7f915 100755
--- a/Dockerfile/ninox.sh
+++ b/Dockerfile/ninox.sh
@@ -44,8 +44,6 @@ sudo docker network create --subnet=172.254.0.0/16 ninoxnet
 ### Running Docker Images :
 
 #### Kafka : 
-
-
 sudo gnome-terminal -e "docker run --net ninoxnet --ip 172.254.0.7 -it --hostname 172.254.0.7 -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=172.254.0.7 --env ADVERTISED_PORT=9092 kafka"
 
 ### Hadoop :
@@ -69,4 +67,5 @@ sudo gnome-terminal -e "docker run --net ninoxnet --ip 172.254.0.5 -it spark"
 #phpmyadmin-like pour mongodb accessible depuis localhost:8080 admin/password 
 sudo docker run -d -p 8080:80 --net ninoxnet --ip 172.254.0.7 --env MONGO_HOST=mongo rocker
 
-#sudo docker run --net ninoxnet --ip 172.254.0.6 -d nginx 
\ No newline at end of file
+#sudo docker run --net ninoxnet --ip 172.254.0.6 -d nginx 
+sudo docker ps
\ No newline at end of file
diff --git a/Spark_Stream/kafka_spark_mongodb.py b/Spark_Stream/kafka_spark_mongodb.py
index c8672a1..4b3dd30 100755
--- a/Spark_Stream/kafka_spark_mongodb.py
+++ b/Spark_Stream/kafka_spark_mongodb.py
@@ -26,6 +26,7 @@ external/kafka-assembly/target/scala-*/spark-streaming-kafka-assembly-*.jar \
 examples/src/main/python/streaming/kafka_wordcount.py \
 localhost:2181 test`
 """
+
 from __future__ import print_function
 
 import sys
@@ -40,11 +41,13 @@ if __name__ == "__main__":
         exit(-1)
 
     sc = SparkContext(appName="PythonStreamingKafkaWordCount")
-    ssc = StreamingContext(sc, 1)
+    ssc = StreamingContext(sc, 10)
+
+    directKafkaStream = KafkaUtils.createDirectStream(ssc, ["incomingData"], {"metadata.broker.list": "172.254.0.7:9092"})
+    
+    lines = directKafkaStream.map(lambda x: x[1])
+    print(lines)
 
-    zkQuorum, topic = sys.argv[1:]
-    kvs = KafkaUtils.createStream(ssc, zkQuorum, "incomingData", {topic: 1})
-    lines = kvs.map(lambda x: x[1])
     counts = lines.flatMap(lambda line: line.split(" ")) \
         .map(lambda word: (word, 1)) \
         .reduceByKey(lambda a, b: a+b)
diff --git a/Spark_Stream/run.sh b/Spark_Stream/run.sh
index 44df542..11ae0d0 100644
--- a/Spark_Stream/run.sh
+++ b/Spark_Stream/run.sh
@@ -1,23 +1,29 @@
-# Launching Spark Docker
-sudo docker run --net ninoxnet --ip 172.254.0.5 -d spark -h sandbox 
+# Installing gnome terminal
+sudo apt-get install -y gnome-terminal
+
+# Run architecture core
+sudo ln -s ~/Ninox/Dockerfile/ninox.sh ninox.sh
+sudo gnome-terminal -e "bash ninox.sh"
+sleep 15
 
 # Creating symbolic link
 sudo ln -s ~/Ninox/kafka_custom_producer/DataProducer.class DataProducer.class
 sudo ln -s ~/Ninox/kafka_custom_producer/DataProducer.java DataProducer.java
 sudo ln -s ~/Ninox/kafka_custom_producer/libs/ libs
 sudo ln -s ~/Ninox/kafka_custom_producer/data.csv data.csv
+sudo ln -s ~/Ninox/kafka_custom_producer/compile.sh compile.sh
 
 export YARN_CONF_DIR="`pwd`/yarn-remote-client"
 export HADOOP_USER_NAME=root
 export SPARK_PUBLIC_DNS=172.254.0.5
 export SPARK_LOCAL_IP=172.254.0.5
 
-sudo gnome-terminal -e "java -cp ".:libs/*" DataProducer"
-sudo gnome-terminal -e "docker run --net ninoxnet --ip 172.254.0.5 -it spark -h sandbox"
+sudo gnome-terminal -e "bash compile.sh"
+# sudo gnome-terminal -e "docker run --net ninoxnet --ip 172.254.0.5 -it spark"
 
 # Run application locally on 8 cores
 sudo ~/spark-2.1.0/bin/spark-submit \
 	--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0 \
 	--master local[4] \
-	~/Ninox/Spark_Stream/kafka_spark_mongodb.py \
+	~/Ninox/Spark_Stream/kafka_spark_mongodb.py 172.254.0.7:2181 incomingData
 	100
\ No newline at end of file
diff --git a/kafka_custom_producer/DataProducer.class b/kafka_custom_producer/DataProducer.class
index 8c08bbcccc5cfeac44ad0446d3c25f17cc9a516d..d94559307d0c115e3b47725fb47f269436aaadf2 100644
GIT binary patch
delta 1252
zcmZ{iUr>})7{-5Rm)-O2w;v8dtFD0{Y$yn<u>$r1t0+>~nkG?TWuR^Z!9oZWS+4!h
z{#f?iH{CSkMKfB>syS0r&Dab#-841bIKy<~Mcz2$R5MN6dsfCP`Yzu0dC&8l_dMr2
z@29K3D$6eZ`}H@#Haweq>CxP8<D<vo0gGM(_gXw?k+Rrl@sMV}O_&3khi!^EXwhf!
zh^Aljs7)!4*{o*3gh9<An>F;9@VLbjCO@e;Z1I%M-3*!i!=e2g;i$zi!=^2swi)4>
zfEbc*Q;E$eW14489yejaw9aTIZMHC_dCsPhDbq?8r!~jVUOrSgBbN}P;!!hO@OD^w
z$R^Qi=DcQA<8LP64XEAz=Xt^VSWQbk@0Iytl3(;(U$Ui{6AsP1<nS^l6>G<a=Z53s
z!<mtI-^}!AW@Ojw==ibpv_qCt3f1oL3MakKeeq;7uR6TO>zX$l-sCNXof;j<q^mOD
zpRY3dw-uqPbY^aH{)cl-n$r&Na6<F0!+X5%{o$)AKEnqNACho5D>lph>--;)__EFa
zm#XWVKR%P5Q1~ZjXB3fC!Pn?y{D7F65o_t;iFQT#=3ehyQTc|l6=;Q?GUD-!R~?w%
zyH#G@d_I)iHz9GGR27ny<UsvJ)CB>G7Ky>aqC<#Q+60T#zmWir2->}CfrG1xT6Hv#
zBQod)uQK7a2dlGIW7NvA;R`CSux9WgYfGzgtc&Oz8%wJN<yfDiM(Aiz@?FF(QM*9c
z4P9b$j=G308fw)G`7_bLRmM#7`UQ@-Iw}ns78tmVnhcv-X1njOuB)>Jp`k@n*j#&u
zE_ZE1lr9st8;*vhYt;U>F-QD@O!{eZTxJtL@iV{5%<W{6C7HP!+~n4Boo9;*s}faq
zyH*wQr<S;0$az&6G(g2vooWzag{mwNzQPavg%)q0Zp`*m%@8r>xJPEER*tRbGii5T
zj>@rBEHrV2IN!+|{V1=rh%1&l#MBP4)yZ!pxh8l?y8bSYb6xlw!rzp&Tf+Y-{6B*8
zv?;}Q6~t9xI#dZe#CB&^MM$dkQd9^L6QWLV1D&c#))RtT=~ga#*e-YbgyKB?SepAZ
zuBKhnq3P5lCCb#~IZ9|{Qi}(r*{T1=N@==!HM{yWg9da7FnM>LRfJdEFNX_nI&=l|
PGP3&rREloV`q=Xy<5%hA

delta 1018
zcmZvbOH7nk6vuz}8)xpv$A^NVcKVPA6cCxA4|#MPM@DUFEv*$_;QQH!edDX8)v<}`
zs!2n>MH4sOw3)aunb1(8u}w&zNn<44xN&1NZcL~#ae>Bw|8JPM@>`s9?)ja2&wtLn
z^T(p!t8)L&T>1-G#Sb1WWG(AF9I`I!T{eW_n=Ws;bi2Im@{VPrM>+3WdORw4&*goW
zO_mQVn>`lN>rus)&}_AA^H@SQG}~Qvguc(x@3Pb51$Kq|cSlp|t*^1tV-I^R`$E4z
zGzY>{4q6U*G;r8*#G{GB;VvaVw0tylZD-BL(ko4pWBH^gqhDuelj<!;gKkq7OOP~F
z2ibYm!8y|s`IOIsVKWd(@OjYW)XRSnY;rP{3BL46@RiTk3>cOM|Ib@G-{%;|4Q92^
z2?l~Or&V<)eNOR><y)UYz6)9-brs)p+UE>OpB%|xU*x&SS(3wpk!cg$iXPdxQlkw&
z2cs5A*;cJyk~i|v=07n71%_8-W5uLT8L#r1VwF1ML^XCyc`y{~EssY>SlpkAP0$tG
zkJaXEB5p@mR!z+)%kz|{+)?UAsL#({jz=dr8rHv@=U~c)b&YxY=AsoL?J3a>Pid+Y
zg)~{B4NvKdsj?6(ECDa&<>H=BjhFqM7@@f!oj>vuKU2%E{6=24*Ts1Xy1grmF<x3v
z%HQB7w@d3O{qh2%gnp9iMgM?@JQkngUUBUd=QE+Xzye$<^+Kw6KFGK=xjpK4j5^Lz
zul|iv&@2s246%Y=H2(9vE<J5h)G1BvT%d!CiVM1qVa<3{{EGOPa^vFvh+kDash<C-
z<8|>H;y0DMEq+J*uHq@>AJCSQ)J}<QXNC?Fq0_`zV>~j#aE^*uda;&OmNrYfrNh!`
zc|#UvN#j0~SyC9w>PJ{dPdu$Z;F>LSK3(#eS?1#<p1B>Y6zG6-Y`XaP)=G>+*Au;H
BwO{}M

diff --git a/kafka_custom_producer/DataProducer.java b/kafka_custom_producer/DataProducer.java
index 80205de..16b6e16 100644
--- a/kafka_custom_producer/DataProducer.java
+++ b/kafka_custom_producer/DataProducer.java
@@ -30,17 +30,24 @@ public class DataProducer {
 
 		br = new BufferedReader(new FileReader(csvFile));
 
+        int i = 1;
 		while ((line = br.readLine()) != null) {
+
 			line += ";";
 		    KeyedMessage<String, String> data = new KeyedMessage<String, String>("incomingData", line);
 		    producer.send(data);
 		
 			try {
+
+                System.out.println("Ligne n° "+i+" envoyé.");
+                System.out.println(line);
+                
 				Thread.sleep(10000);
 			} catch(InterruptedException ex) {
 			    Thread.currentThread().interrupt();
 			}
-		    
+
+            i = i+1;
 		}
 	
 	} catch (FileNotFoundException e) {
-- 
2.7.4

